{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>第一种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-ca0544dbc09e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-ca0544dbc09e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <font face=\"黑体\">我是黑体字</font>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from keras import backend as K\n",
    "def f1(y_true, y_pred):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 第二种方法\n",
    "<h2> 直接调用 from sklearn.metrics import f1_score\n",
    "<h2> 下面是f1_score的函数说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def f1_score(y_true, y_pred, labels=None, pos_label=1, average='binary',\n",
    "             sample_weight=None):\n",
    "    \"\"\"Compute the F1 score, also known as balanced F-score or F-measure\n",
    "\n",
    "    The F1 score can be interpreted as a weighted average of the precision and\n",
    "    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "    The relative contribution of precision and recall to the F1 score are\n",
    "    equal. The formula for the F1 score is::\n",
    "\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    In the multi-class and multi-label case, this is the weighted average of\n",
    "    the F1 score of each class.\n",
    "    #在多类别和多标签的情况下，这是每个类别的F1分数的加权平均值。\n",
    "\n",
    "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : \n",
    "    \n",
    "        1d array-like, or label indicator array / sparse matrix\n",
    "        Ground truth (correct) target values.\n",
    "\n",
    "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
    "        Estimated targets as returned by a classifier.\n",
    "\n",
    "    labels : list, optional\n",
    "        The set of labels to include when ``average != 'binary'``, and their\n",
    "        order if ``average is None``. Labels present in the data can be\n",
    "        excluded, for example to calculate a multiclass average ignoring a\n",
    "        majority negative class, while labels not present in the data will\n",
    "        result in 0 components in a macro average. For multilabel targets,\n",
    "        labels are column indices. By default, all labels in ``y_true`` and\n",
    "        ``y_pred`` are used in sorted order.\n",
    "\n",
    "        .. versionchanged:: 0.17\n",
    "           parameter *labels* improved for multiclass problem.\n",
    "\n",
    "    pos_label : str or int, 1 by default\n",
    "        The class to report if ``average='binary'`` and the data is binary.\n",
    "        If the data are multiclass or multilabel, this will be ignored;\n",
    "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
    "        scores for that label only.\n",
    "\n",
    "    average : string, [None, 'binary' (default), 'micro', 'macro', 'samples', \\\n",
    "                       'weighted']\n",
    "              #参数默认的是二进制\n",
    "              \n",
    "        This parameter is required for multiclass/multilabel targets.\n",
    "        If ``None``, the scores for each class are returned. Otherwise, this\n",
    "        determines the type of averaging performed on the data:\n",
    "\n",
    "        ``'binary'``:\n",
    "            Only report results for the class specified by ``pos_label``.\n",
    "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
    "        ``'micro'``:\n",
    "            Calculate metrics globally by counting the total true positives,\n",
    "            false negatives and false positives.\n",
    "        ``'macro'``:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "        ``'weighted'``:\n",
    "            Calculate metrics for each label, and find their average, weighted\n",
    "            by support (the number of true instances for each label). This\n",
    "            alters 'macro' to account for label imbalance; it can result in an\n",
    "            F-score that is not between precision and recall.\n",
    "        ``'samples'``:\n",
    "            Calculate metrics for each instance, and find their average (only\n",
    "            meaningful for multilabel classification where this differs from\n",
    "            :func:`accuracy_score`).\n",
    "\n",
    "    sample_weight : array-like of shape = [n_samples], optional\n",
    "        Sample weights.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f1_score : float or array of float, shape = [n_unique_labels]\n",
    "        F1 score of the positive class in binary classification or weighted\n",
    "        average of the F1 scores of each class for the multiclass task.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] `Wikipedia entry for the F1-score\n",
    "           <https://en.wikipedia.org/wiki/F1_score>`_\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.metrics import f1_score\n",
    "    >>> y_true = [0, 1, 2, 0, 1, 2]\n",
    "    >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
    "    >>> f1_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS\n",
    "    0.26...\n",
    "    >>> f1_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS\n",
    "    0.33...\n",
    "    >>> f1_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS\n",
    "    0.26...\n",
    "    >>> f1_score(y_true, y_pred, average=None)\n",
    "    array([ 0.8,  0. ,  0. ])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    return fbeta_score(y_true, y_pred, 1, labels=labels,\n",
    "                       pos_label=pos_label, average=average,\n",
    "                       sample_weight=sample_weight)\n",
    "\n",
    "准确率（P）： TP/ (TP+FP) \n",
    "召回率(R)： TP(TP + FN)\n",
    " \n",
    "对于数据测试结果有下面4种情况：\n",
    "真阳性（TP）: 预测为正， 实际也为正\n",
    "假阳性（FP）: 预测为正， 实际为负\n",
    "假阴性（FN）: 预测为负，实际为正\n",
    "真阴性（TN）: 预测为负， 实际也为负\n",
    "\n",
    "f1_score中关于参数average的用法描述:\n",
    "'micro':通过先计算总体的TP，FN和FP的数量，再计算F1 \n",
    "'macro':分布计算每个类别的F1，然后做平均（各类别F1的权重相同）\n",
    "\n",
    " \n",
    "macro其实就是先计算出每个类别的F1值，然后去平均，比如下面多分类问题，\n",
    "总共有1,2,3,4这4个类别，我们可以先算出1的F1，2的F1，3的F1，4的F1,然后再取平均（F1+F1+F1+F1）/4\n",
    "\n",
    "如果是二分类问题则选择参数‘binary’；\n",
    "如果考虑类别的不平衡性，需要计算类别的加权平均，则使用‘weighted’；\n",
    "如果不考虑类别的不平衡性，计算宏平均，则使用‘macro’。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>第三种方法：\n",
    "<h2> from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def f1_score(y_true, y_pred, average='micro', suffix=False):\n",
    "    \"\"\"Compute the F1 score.\n",
    "\n",
    "    The F1 score can be interpreted as a weighted average of the precision and\n",
    "    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "    The relative contribution of precision and recall to the F1 score are\n",
    "    equal. The formula for the F1 score is::\n",
    "\n",
    "        F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    Args:\n",
    "        y_true : 2d array. Ground truth (correct) target values.\n",
    "        y_pred : 2d array. Estimated targets as returned by a tagger.\n",
    "\n",
    "    Returns:\n",
    "        score : float.\n",
    "\n",
    "    Example:\n",
    "        >>> from seqeval.metrics import f1_score\n",
    "        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "        >>> f1_score(y_true, y_pred)\n",
    "        0.50\n",
    "    \"\"\"\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
